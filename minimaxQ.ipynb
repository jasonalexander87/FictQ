{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minimaxQ.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg.linalg import solve\n",
        "import numpy as np\n",
        "import random \n",
        "from scipy.optimize import linprog\n",
        "\n",
        "#R = np.array([[1.0,-1.0],[-1.0,1.0]])\n",
        "#C = np.array([[-1.0,1.0],[1.0,-1.0]])\n",
        "R = np.array([[-2.0,3.0],[3.0,-4.0]])\n",
        "C = np.array([[2.0,-3.0],[-3.0,4.0]])\n",
        "moves = np.array([0,1])\n",
        "#Init Values PLAYER1\n",
        "valueQ_p1 = np.array([[1.0,1.0],[1.0,1.0]])\n",
        "valueV_p1 = 1.0\n",
        "valueP_p1 = np.array([0.9,0.1])\n",
        "#Init Values PLAYER2\n",
        "valueQ_p2 = np.array([[1.0,1.0],[1.0,1.0]])\n",
        "valueV_p2 = 1.0\n",
        "valueP_p2 = np.array([0.9,0.1])\n",
        "\n",
        "#VALUE OF THE GAME\n",
        "value_game_p1 = 1.0\n",
        "value_game_p2 = 1.0\n",
        "\n",
        "alpha = 1.0\n",
        "explor = 0.1\n",
        "notexplor = 0.9\n",
        "\n",
        "def getRew_p1(action1,action2):\n",
        "  return R[action1][action2]\n",
        "\n",
        "def getRew_p2(action1,action2):\n",
        "  return C[action1][action2]\n",
        "\n",
        "def chooseMove(movs,wei):\n",
        "  return random.choices(movs,weights=wei,k=1)[0]\n",
        "\n",
        "def updateQ_p1(a,rew,mov,movO):\n",
        "  return (1-a)*valueQ_p1[mov][movO] + a*rew\n",
        "\n",
        "def updateQ_p2(a,rew,mov,movO):\n",
        "  return (1-a)*valueQ_p2[mov][movO] + a*rew\n",
        "\n",
        "def solveLP(Q):\n",
        "  c = np.array([[0.0,0.0,1.0]])\n",
        "  b_ub = np.array([0.0,0.0])\n",
        "  b_eq = np.array([1.0])\n",
        "  A_eq = np.array([[1.0,1.0,0.0]])\n",
        "  x0_bounds = (0.0,1.0)\n",
        "  x1_bounds = (0.0,1.0)\n",
        "  x2_bounds = (-1.0, 1.0)  # +/- np.inf can be used instead of None\n",
        "  bounds = [x0_bounds, x1_bounds, x2_bounds]\n",
        "\n",
        "  test1 = []\n",
        "  test1.append(Q[0][0])\n",
        "  test1.append(Q[1][0])\n",
        "  test1.append(1)\n",
        "\n",
        "  test2 = []\n",
        "  test2.append(Q[0][1])\n",
        "  test2.append(Q[1][1])\n",
        "  test2.append(1)\n",
        "\n",
        "  A_ub = []\n",
        "  A_ub.append(test1)\n",
        "  A_ub.append(test2)\n",
        "\n",
        "  result = linprog(c,A_ub=A_ub,b_ub=b_ub,A_eq=A_eq,b_eq=b_eq,bounds=bounds)\n",
        "\n",
        "  return result.x[0],result.x[1]\n",
        "\n",
        "\n",
        "def computeValue2_p1(policy,Q):\n",
        "  res1 = policy[0]*Q[0][0] + policy[1]*Q[1][0]\n",
        "  res2 = policy[0]*Q[0][1] + policy[1]*Q[1][1]\n",
        "  return min(res1,res2)\n",
        "\n",
        "def computeValue2_p2(policy,Q):\n",
        "  res1 = policy[0]*Q[0][0] + policy[1]*Q[1][0]\n",
        "  res2 = policy[0]*Q[0][1] + policy[1]*Q[1][1]\n",
        "  return max(res1,res2)\n",
        "\n",
        "def updateAlpha(alhpa):\n",
        "  decay = 0.95\n",
        "  return alpha*decay\n",
        "\n",
        "iterations = 0\n",
        "while iterations < 1000 :\n",
        "#RANDOM PLAY OR POLICY PLAY \n",
        " play_random1 = random.random()\n",
        " play_random2 = random.random()\n",
        " if(play_random1 > explor):\n",
        "   weights1 = valueP_p1\n",
        " else:\n",
        "   weights1 = np.array([0.5,0.5])\n",
        " if(play_random2 > explor):\n",
        "   weights2 = valueP_p2\n",
        " else:\n",
        "   weights2 = np.array([0.5,0.5])\n",
        "\n",
        "#EPILOGH KINHSHS GIA KA8E PAIXTH\n",
        " move_p1 = chooseMove(moves,weights1)  \n",
        " move_p2 = chooseMove(moves,weights2)\n",
        " \n",
        " reward1 = getRew_p1(move_p1,move_p2)\n",
        " reward2 = getRew_p2(move_p2,move_p1)\n",
        "#UPDATE Q VALUES\n",
        " valueQ_p1[move_p1][move_p2] = updateQ_p1(alpha,reward1,move_p1,move_p2)\n",
        " valueQ_p2[move_p2][move_p1] = updateQ_p2(alpha,reward2,move_p2,move_p1)\n",
        "\n",
        "#YPOLOGISMOS TOU NEOU POLICY GIA KA8E PAIXTH\n",
        " valueP_p1[0], valueP_p1[1] = solveLP(valueQ_p1)\n",
        " valueP_p2[0], valueP_p2[1] = solveLP(valueQ_p2)\n",
        "#UPDATE VALUE OF THE GAME FOR PLAYERS\n",
        " value_game_p1 = computeValue2_p1(valueP_p1,valueQ_p1)\n",
        " value_game_p2 = computeValue2_p2(valueP_p2,valueQ_p2)\n",
        "#UPDATE ALPHA\n",
        " alpha = updateAlpha(alpha)\n",
        "\n",
        " iterations = iterations + 1\n",
        " \n",
        "\n",
        "print(value_game_p1)\n",
        "print(value_game_p2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdIwSvpuiMfi",
        "outputId": "1eac103e-b482-4c25-c690-79e591c6b9d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.17849722824070025\n",
            "0.14257229830081908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg.linalg import solve\n",
        "import numpy as np\n",
        "import random \n",
        "from scipy.optimize import linprog\n",
        "\n",
        "#FUNCTION TO CHOOSE MOVE BASED ON WEIGTHS\n",
        "def chooseMove(movs,wei):\n",
        "  return random.choices(movs,weights=wei,k=1)[0]\n",
        "\n",
        "#FUNCTION TO RETURN REWARD BASED ON ACTIONS AND STATE FOR PLAYER 1\n",
        "def getRew_p1(action1,action2,state):\n",
        "  if state == 0:\n",
        "    return R_0[action1][action2]\n",
        "  elif state == 1:\n",
        "    return R_1[action1][action2]\n",
        "  else:\n",
        "    return R_2[action1][action2]    \n",
        "  \n",
        "#FUNCTION TO RETURN REWARD BASED ON ACTIONS AND STATE FOR PLAYER 2\n",
        "def getRew_p2(action1,action2,state):\n",
        "  if state == 0:\n",
        "    return C_0[action1][action2]\n",
        "  elif state == 1:\n",
        "    return C_1[action1][action2]\n",
        "  else:\n",
        "    return C_2[action1][action2] \n",
        "\n",
        "#FUNCTION TO UPDATE Q VALUE BASED ON PARAMETERS FOR PLAYER 1\n",
        "def updateQ_p1(a,rew,mov,movO,state):\n",
        "  if state == 0:\n",
        "    return (1-a)*valueQ_state1_p1[mov][movO] + a*rew\n",
        "  elif state == 1:\n",
        "    return (1-a)*valueQ_state2_p1[mov][movO] + a*rew\n",
        "  else:\n",
        "    return (1-a)*valueQ_state3_p1[mov][movO] + a*rew  \n",
        "\n",
        "#FUNCTION TO UPDATE Q VALUE BASED ON PARAMETERS FOR PLAYER 2\n",
        "def updateQ_p2(a,rew,mov,movO,state):\n",
        "  if state == 0:\n",
        "    return (1-a)*valueQ_state1_p2[mov][movO] + a*rew\n",
        "  elif state == 1:\n",
        "    return (1-a)*valueQ_state2_p2[mov][movO] + a*rew\n",
        "  else:\n",
        "    return (1-a)*valueQ_state3_p2[mov][movO] + a*rew \n",
        "\n",
        "#SOLVE LINERAR PROGRAM TO FIND POLICY \n",
        "def solveLP(state,player):\n",
        "  if state == 0 and player == 0:\n",
        "    Q = valueQ_state1_p1\n",
        "  elif state == 1 and player == 0:\n",
        "    Q = valueQ_state2_p1\n",
        "  elif state == 2 and player == 0:\n",
        "    Q = valueQ_state3_p1\n",
        "  elif state == 0 and player == 1:\n",
        "    Q = valueQ_state1_p2\n",
        "  elif state == 1 and player == 1:\n",
        "    Q = valueQ_state2_p2\n",
        "  else:\n",
        "    Q = valueQ_state3_p2      \n",
        "\n",
        "  c = np.array([[0.0,0.0,0.0,0.0,1.0]])\n",
        "  b_ub = np.array([0.0,0.0,0.0,0.0])\n",
        "  b_eq = np.array([1.0])\n",
        "  A_eq = np.array([[1.0,1.0,1.0,1.0,0.0]])\n",
        "  x0_bounds = (0.0,1.0)\n",
        "  x1_bounds = (0.0,1.0)\n",
        "  x2_bounds = (0.0,1.0)\n",
        "  x3_bounds = (0.0,1.0)\n",
        "  x4_bounds = (-1.0, 1.0)  # +/- np.inf can be used instead of None\n",
        "  bounds = [x0_bounds, x1_bounds, x2_bounds, x3_bounds, x4_bounds]\n",
        "\n",
        "  test1 = []\n",
        "  test1.append(Q[0][0])\n",
        "  test1.append(Q[1][0])\n",
        "  test1.append(Q[2][0])\n",
        "  test1.append(Q[3][0])\n",
        "  test1.append(1)\n",
        "\n",
        "  test2 = []\n",
        "  test2.append(Q[0][1])\n",
        "  test2.append(Q[1][1])\n",
        "  test2.append(Q[2][1])\n",
        "  test2.append(Q[3][1])\n",
        "  test2.append(1)\n",
        "\n",
        "  test3 = []\n",
        "  test3.append(Q[0][2])\n",
        "  test3.append(Q[1][2])\n",
        "  test3.append(Q[2][2])\n",
        "  test3.append(Q[3][2])\n",
        "  test3.append(1)\n",
        "\n",
        "  test4 = []\n",
        "  test4.append(Q[0][3])\n",
        "  test4.append(Q[1][3])\n",
        "  test4.append(Q[2][3])\n",
        "  test4.append(Q[3][3])\n",
        "  test4.append(1)\n",
        "\n",
        "  A_ub = []\n",
        "  A_ub.append(test1)\n",
        "  A_ub.append(test2)\n",
        "  A_ub.append(test3)\n",
        "  A_ub.append(test4)\n",
        "\n",
        "  result = linprog(c,A_ub=A_ub,b_ub=b_ub,A_eq=A_eq,b_eq=b_eq,bounds=bounds)\n",
        "\n",
        "  return result.x[0],result.x[1],result.x[2],result.x[3]\n",
        "\n",
        "#COMPUTE THE VALUE OF THE STATE\n",
        "def computeValue_p1(state):\n",
        "  if state == 0:\n",
        "    Q = valueQ_state1_p1\n",
        "    P = valueP_p1[0]\n",
        "  elif state == 1:\n",
        "    Q = valueQ_state2_p1\n",
        "    P = valueP_p1[1]\n",
        "  else:\n",
        "    Q = valueQ_state3_p1\n",
        "    P = valueP_p1[2]\n",
        "  \n",
        "  res1 = P[0]*Q[0][0] + P[1]*Q[1][0] + P[2]*Q[2][0] + P[3]*Q[3][0] \n",
        "  res2 = P[0]*Q[0][1] + P[1]*Q[1][1] + P[2]*Q[2][1] + P[3]*Q[3][1]  \n",
        "  res3 = P[0]*Q[0][2] + P[1]*Q[1][2] + P[2]*Q[2][2] + P[3]*Q[3][2] \n",
        "  res4 = P[0]*Q[0][3] + P[1]*Q[1][3] + P[2]*Q[2][3] + P[3]*Q[3][3] \n",
        "\n",
        "  array = []\n",
        "  array.append(res1)\n",
        "  array.append(res2)\n",
        "  array.append(res3)\n",
        "  array.append(res4)\n",
        "  result = max(array)\n",
        "\n",
        "  return result\n",
        "\n",
        "def computeValue_p2(state):\n",
        "  if state == 0:\n",
        "    Q = valueQ_state1_p2\n",
        "    P = valueP_p2[0]\n",
        "  elif state == 1:\n",
        "    Q = valueQ_state2_p2\n",
        "    P = valueP_p2[1]\n",
        "  elif state == 2:\n",
        "    Q = valueQ_state3_p2\n",
        "    P = valueP_p2[2]\n",
        "  \n",
        "  res1 = P[0]*Q[0][0] + P[1]*Q[1][0] + P[2]*Q[2][0] + P[3]*Q[3][0] \n",
        "  res2 = P[0]*Q[0][1] + P[1]*Q[1][1] + P[2]*Q[2][1] + P[3]*Q[3][1]  \n",
        "  res3 = P[0]*Q[0][2] + P[1]*Q[1][2] + P[2]*Q[2][2] + P[3]*Q[3][2] \n",
        "  res4 = P[0]*Q[0][3] + P[1]*Q[1][3] + P[2]*Q[2][3] + P[3]*Q[3][3] \n",
        "\n",
        "  array = []\n",
        "  array.append(res1)\n",
        "  array.append(res2)\n",
        "  array.append(res3)\n",
        "  array.append(res4)\n",
        "  result = min(array)\n",
        "\n",
        "  return result   \n",
        "\n",
        "#UPDATE ALPHA BASED ON a = 1/k NOT DECAY THIS TIME !!!\n",
        "def updateAlphaKappa(player,mov,mov0,state):\n",
        "  if state == 0 and player == 0:\n",
        "    alpha = alpha_s1_p1\n",
        "    kappa = kappa_state_1_p1\n",
        "  elif state == 1 and player == 0:\n",
        "    alpha = alpha_s2_p1\n",
        "    kappa = kappa_state_2_p1\n",
        "  elif state == 2 and player == 0:\n",
        "    alpha = alpha_s3_p1\n",
        "    kappa = kappa_state_3_p1\n",
        "  elif state == 0 and player == 1:\n",
        "    alpha = alpha_s1_p2\n",
        "    kappa = kappa_state_1_p2\n",
        "  elif state == 1 and player == 1:\n",
        "    alpha = alpha_s2_p2\n",
        "    kappa = kappa_state_2_p2\n",
        "  else:\n",
        "    alpha = alpha_s3_p2\n",
        "    kappa = kappa_state_3_p2\n",
        "\n",
        "  kappa_new = kappa[mov][mov0] +1\n",
        "  kappa[mov][mov0] = kappa_new\n",
        "  alpha[mov][mov0] = 1/kappa[mov][mov0] \n",
        "\n",
        "\n",
        "#PAYOFF MATRIX S_0 STATE 0 \n",
        "R_0 = np.array([[0.0,0.0,-0.5,-1.0],[0.0,0.0,-0.5,-1.0],[0.5,0.5,0.0,0.5],[1.0,1.0,-0.5,0.0]])\n",
        "C_0 = R_0*(-1)\n",
        "#PAYOFF MATRIX S_1 STATE 1\n",
        "R_1 = np.array([[0.0,0.0,-0.5,1.0],[0.0,0.0,-0.5,-1.0],[0.5,0.5,0.0,0.0],[1.0,-1.0,0.0,0.0]])\n",
        "C_1 = R_1*(-1)\n",
        "#PAYOFF MATRIX S_2 STATE 2\n",
        "R_2 = np.array([[0.0,0.0,-0.5,-1.0],[0.0,0.0,-0.5,1.0],[0.5,0.5,0.0,0.0],[-1.0,1.0,0.0,0.0]])\n",
        "C_2 = R_2*(-1)\n",
        "#MOVES 0=R 1=L 2=B 3=A POSSIBLE MOVES\n",
        "moves = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])\n",
        "movs = np.array([0,1,2,3])\n",
        "\n",
        "\n",
        "#STATES 0,1,2,3 STATE 3 IS TERMINAL \n",
        "states = np.array([0,1,2,3])\n",
        "#TRANSITION FROM S_0\n",
        "T_0 = np.array([[0,2,1,1],[1,0,2,3],[2,1,0,0],[3,1,0,3]])\n",
        "#TRANSITION FROM S_1\n",
        "T_1 = np.array([[1,0,2,3],[2,1,0,0],[0,2,1,1],[0,3,1,3]])\n",
        "#TRANSITION FROM S_2\n",
        "T_2 = np.array([[2,1,0,0],[0,2,1,1],[1,0,2,2],[1,1,2,3]])\n",
        "\n",
        "#Init Values PLAYER1 Q VALUES FOR EACH STATE AND ACTION PROFILE\n",
        "valueQ_state1_p1 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "valueQ_state2_p1 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "valueQ_state3_p1 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "#VALUE OF EACH STATE PLAYER 1\n",
        "valueV_p1 = np.array([1.0,1.0,1.0])\n",
        "#POLICY VALUE ONE FOR EACH STATE\n",
        "valueP_p1 = np.array([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25]])\n",
        "\n",
        "#Init Values PLAYER2\n",
        "valueQ_state1_p2 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "valueQ_state2_p2 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "valueQ_state3_p2 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "\n",
        "valueV_p2 = np.array([1.0,1.0,1.0])\n",
        "valueP_p2 = np.array([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25]])\n",
        "\n",
        "#ALPHA VALUE FOR EACH STATE AND ACTION PROFILE\n",
        "alpha_s1_p1 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "alpha_s2_p1 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "alpha_s3_p1 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "#K values for each state action profile\n",
        "kappa_state_1_p1 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "kappa_state_2_p1 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "kappa_state_3_p1 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "\n",
        "#ALPHA VALUE FOR EACH STATE AND ACTION PROFILE player 2\n",
        "alpha_s1_p2 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "alpha_s2_p2 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "alpha_s3_p2 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "#K values for each state action profile player 2\n",
        "kappa_state_1_p2 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "kappa_state_2_p2 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "kappa_state_3_p2 = np.array([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0]])\n",
        "\n",
        "#STARTING STATE\n",
        "current_state = 0\n",
        "explor = 0.1\n",
        "\n",
        "iterations = 1\n",
        "\n",
        "while iterations < 1000 :\n",
        " if current_state == 3:\n",
        "   current_state = 0 \n",
        "#RANDOM PLAY OR POLICY PLAY \n",
        " play_random1 = random.random()\n",
        " play_random2 = random.random()\n",
        " if(play_random1 > explor):\n",
        "   weights1 = valueP_p1[current_state]\n",
        " else:\n",
        "   weights1 = np.array([0.25,0.25,0.25,0.25])\n",
        " if(play_random2 > explor):\n",
        "   weights2 = valueP_p2[current_state]\n",
        " else:\n",
        "   weights2 = np.array([0.25,0.25,0.25,0.25])\n",
        "#CHOOSE MOVES  \n",
        " move_p1 = chooseMove(movs,weights1)  \n",
        " move_p2 = chooseMove(movs,weights2) \n",
        "#GET REWARD\n",
        " reward1 = getRew_p1(move_p1,move_p2,current_state)\n",
        " reward2 = getRew_p2(move_p2,move_p1,current_state)\n",
        "#UPDATE Q VALUES\n",
        " if current_state == 0:\n",
        "   alpha_1 = alpha_s1_p1[move_p1][move_p2]\n",
        "   alpha_2 = alpha_s1_p2[move_p2][move_p1]\n",
        "\n",
        "   updateAlphaKappa(0,move_p1,move_p2,current_state)\n",
        "   updateAlphaKappa(1,move_p2,move_p1,current_state)\n",
        "\n",
        "   valueQ_state1_p1[move_p1][move_p2] = updateQ_p1(alpha_1,reward1,move_p1,move_p2,current_state)\n",
        "   valueQ_state1_p2[move_p2][move_p1] = updateQ_p2(alpha_2,reward2,move_p2,move_p1,current_state)\n",
        " elif current_state == 1:\n",
        "   alpha_1 = alpha_s2_p1[move_p1][move_p2]\n",
        "   alpha_2 = alpha_s2_p2[move_p2][move_p1]\n",
        "\n",
        "   updateAlphaKappa(0,move_p1,move_p2,current_state)\n",
        "   updateAlphaKappa(1,move_p2,move_p1,current_state)\n",
        "\n",
        "   valueQ_state2_p1[move_p1][move_p2] = updateQ_p1(alpha_1,reward1,move_p1,move_p2,current_state)\n",
        "   valueQ_state2_p2[move_p2][move_p1] = updateQ_p2(alpha_2,reward2,move_p2,move_p1,current_state)\n",
        " else:\n",
        "   alpha_1 = alpha_s3_p1[move_p1][move_p2]\n",
        "   alpha_2 = alpha_s3_p2[move_p2][move_p1]\n",
        "\n",
        "   updateAlphaKappa(0,move_p1,move_p2,current_state)\n",
        "   updateAlphaKappa(1,move_p2,move_p1,current_state)\n",
        "\n",
        "   valueQ_state3_p1[move_p1][move_p2] = updateQ_p1(alpha_1,reward1,move_p1,move_p2,current_state)\n",
        "   valueQ_state3_p2[move_p2][move_p1] = updateQ_p2(alpha_2,reward2,move_p2,move_p1,current_state)  \n",
        "#FIND POLICY\n",
        "\n",
        " valueP_p1[current_state][0],valueP_p1[current_state][1],valueP_p1[current_state][2],valueP_p1[current_state][3] = solveLP(current_state,0)\n",
        " valueP_p2[current_state][0],valueP_p2[current_state][1],valueP_p2[current_state][2],valueP_p2[current_state][3] = solveLP(current_state,1)\n",
        "\n",
        " valueV_p1[current_state] = computeValue_p1(current_state)\n",
        " valueV_p2[current_state] = computeValue_p2(current_state)\n",
        "\n",
        " if current_state == 0:\n",
        "    current_state = T_0[move_p1][move_p2]\n",
        " elif current_state == 1:\n",
        "    current_state = T_1[move_p1][move_p2]\n",
        " elif current_state == 2:\n",
        "    current_state = T_2[move_p1][move_p2]\n",
        " else:\n",
        "    current_state = 0   \n",
        "\n",
        " iterations = iterations +1\n",
        "\n",
        "\n",
        "print(valueQ_state1_p1[0])\n",
        "print(valueQ_state1_p1[1])\n",
        "print(valueQ_state1_p1[2])\n",
        "print(valueQ_state1_p2[0])\n",
        "print(valueQ_state1_p2[1])\n",
        "print(valueQ_state1_p2[2])\n"
      ],
      "metadata": {
        "id": "PI5s_zCQjtSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6adc4551-17c0-4d17-8724-3f786d13851a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.   0.  -0.5 -1. ]\n",
            "[ 0.   0.  -0.5 -1. ]\n",
            "[0.5 0.5 0.  0.5]\n",
            "[0.  0.  0.5 1. ]\n",
            "[0.  0.  0.5 1. ]\n",
            "[-0.5 -0.5  0.  -0.5]\n"
          ]
        }
      ]
    }
  ]
}